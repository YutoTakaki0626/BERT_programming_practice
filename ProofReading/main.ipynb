{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6209f06a-27f6-46dc-931d-9a144111e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eddb60-a027-422b-9cd0-abe122a1c222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.5.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (4.5.0)\n",
      "Requirement already satisfied: fugashi==1.1.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: ipadic==1.0.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: pytorch-lightning==1.2.7 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (1.2.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2021.4.4)\n",
      "Requirement already satisfied: packaging in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (1.19.2)\n",
      "Requirement already satisfied: requests in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (0.10.3)\n",
      "Requirement already satisfied: filelock in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (4.59.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (0.18.2)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (0.4.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (2.4.0)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (5.3.1)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.4 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (1.7.0)\n",
      "Requirement already satisfied: aiohttp in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (3.7.4.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.11.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.23.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.3.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.14.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning==1.2.7) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning==1.2.7) (0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (5.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
      "Requirement already satisfied: click in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/yuto/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bb845c-40ec-4832-b24d-5c7bcbe49048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015c3778-7f90-430e-a64a-35f3534c07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proofreading_tokenizer import SC_tokenizer\n",
    "\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758bad4-98fd-4d03-827d-886c074c3627",
   "metadata": {},
   "source": [
    "## encode_plus_tagged\n",
    "ファインチューニング時に使用。\n",
    "誤変換を含む文章と正しい文章を入力とし、\n",
    "符号化を行いBERTに入力できる形式にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f6f5f1-89a1-4ec5-830b-ce1b07a8634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "wrong_text = '優勝トロフィーを変換した'\n",
    "correct_text = '優勝トロフィーを返還した'\n",
    "encoding = tokenizer.encode_plus_tagged(\n",
    "    wrong_text, correct_text, max_length=12\n",
    ")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f326aa-d9d4-44c3-8189-8101b2252444",
   "metadata": {},
   "source": [
    "## encode_plus_untagged\n",
    "文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a51fd62-5f9c-4c43-abac-4862f993d193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "wrong_text = '優勝トロフィーを変換した'\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    wrong_text, return_tensors='pt'\n",
    ")\n",
    "print('# encoding')\n",
    "print(encoding)\n",
    "print('# spans')\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebaab6-8bf1-475e-a1d2-4888efd603ee",
   "metadata": {},
   "source": [
    "## convert_bert_output_to_text\n",
    "推論時に使用。\n",
    "文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
    "そこから、BERTによって予測された文章に変換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e449d68-f545-4a15-903a-37dbfd6afeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "優勝トロフィーを返還した\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
    "predicted_text = tokenizer.convert_bert_output_to_text(\n",
    "    wrong_text, predicted_labels, spans\n",
    ")\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9cc69b-cb4f-4e25-939e-5cc21099012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707357b-4d7f-46e9-87ad-03d9e311cb92",
   "metadata": {},
   "source": [
    "### 学習①"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea1521c4-c98c-4f68-8751-178088bc8835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'優勝トロフィーを獲得した。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '優勝トロフィーを変換した。'\n",
    "\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(**encoding)\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).numpy().tolist()\n",
    "    \n",
    "predict_text = tokenizer.convert_bert_output_to_text(\n",
    "    text, labels_predicted, spans\n",
    ")\n",
    "predict_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47d92-9731-4172-9975-2a47ac17423c",
   "metadata": {},
   "source": [
    "### 学習②"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a0528a-fcec-4ae6-bb68-cf085384c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'wrong_text': '優勝トロフィーを変換した。',\n",
    "        'correct_text': '優勝トロフィーを返還した。',\n",
    "    },\n",
    "    {\n",
    "        'wrong_text': '人と森は強制している。',\n",
    "        'correct_text': '人と森は共生している。',\n",
    "    }\n",
    "]\n",
    "\n",
    "max_length=32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    wrong_text = sample['wrong_text']\n",
    "    correct_text = sample['correct_text']\n",
    "    encoding = tokenizer.encode_plus_tagged(\n",
    "        wrong_text, correct_text, max_length=max_length\n",
    "    )\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "    \n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    encoding = { k: v for k, v in batch.items() }\n",
    "    output = bert_mlm(**encoding)\n",
    "    loss = output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b1c8a34-b323-44cc-b01e-f1cfe039022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   243  100   243    0     0    820      0 --:--:-- --:--:-- --:--:--   818\n",
      "100   268  100   268    0     0    738      0 --:--:-- --:--:-- --:--:--   738\n",
      "100 64.9M  100 64.9M    0     0  2042k      0  0:00:32  0:00:32 --:--:-- 3646k\n",
      "x jwtd/\n",
      "x jwtd/train.jsonl\n",
      "x jwtd/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
    "!tar zxvf JWTD.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7b1ed9-022e-491f-bbdb-3a1915a5a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習と検証用のデータセット：\n",
      "- 漢字誤変換の総数：235490\n",
      "- トークンの対応関係のつく文章の総数: 172883\n",
      "  (全体の73%)\n",
      "テスト用のデータセット：\n",
      "- 漢字誤変換の総数：3061\n",
      "- トークンの対応関係のつく文章の総数: 2263\n",
      "  (全体の74%)\n"
     ]
    }
   ],
   "source": [
    "from dataset import create_dataset\n",
    "\n",
    "# データのロード\n",
    "train_df = pd.read_json(\n",
    "    './jwtd/train.jsonl', orient='records', lines=True\n",
    ")\n",
    "test_df = pd.read_json(\n",
    "    './jwtd/test.jsonl', orient='records', lines=True\n",
    ")\n",
    "\n",
    "print('学習と検証用のデータセット：')\n",
    "dataset = create_dataset(train_df)\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.8)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:]\n",
    "\n",
    "print('テスト用のデータセット：')\n",
    "dataset_test = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7f295ec-e1a5-44af-8887-9f08e6cf72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138306/138306 [01:07<00:00, 2056.13it/s]\n",
      "100%|██████████| 34577/34577 [00:16<00:00, 2156.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataset_loader import create_dataset_for_loader\n",
    "\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "max_length = 32\n",
    "dataset_train_for_loader = create_dataset_for_loader(\n",
    "    tokenizer, dataset_train, max_length\n",
    ")\n",
    "dataset_val_for_loader = create_dataset_for_loader(\n",
    "    tokenizer, dataset_val, max_length\n",
    ")\n",
    "\n",
    "# データローダの作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a3089d3-1e37-4792-bb33-7459d57d9134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | bert_mlm | BertForMaskedLM | 110 M \n",
      "---------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.604   Total estimated model params size (MB)\n",
      "/Users/yuto/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuto/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7142ed45fe0e4a50bdc31b954a95b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuto/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from Lightning_model import BertForMaskedLM_pl\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bec2f7-1505-4795-ac48-eba0c6549b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
    "    'メールに明日の会議の史料を添付した。',\n",
    "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
    "    '突然、子供が帰省を発した。'\n",
    "]\n",
    "\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
    "bert_mlm = model.bert_mlm\n",
    "\n",
    "for text in text_list:\n",
    "    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n",
    "    print('---')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'出力：{predict_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc1860-f8c9-4f67-97a0-f333cc6a921d",
   "metadata": {},
   "source": [
    "### テストデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728583f-b881-421b-9198-0800ca94ac94",
   "metadata": {},
   "source": [
    "####　・予測が完全に一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b504d-0e70-4a3b-842c-1c6b74d27e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_num = 0\n",
    "for sample in tqdm(dataset_test):\n",
    "    wrong_text = sample['wrong_text']\n",
    "    correct_text = sample['correct_text']\n",
    "    predict_text = predict(wrong_text, tokenizer, bert_mlm)\n",
    "    \n",
    "    if correct_text == predicted_text:\n",
    "        correct_num += 1\n",
    "\n",
    "print(f'Accuracy: {correct_num/len(dataset_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af6985-167a-4ffe-88a0-0139ba89ad87",
   "metadata": {},
   "source": [
    "####　・誤変換の漢字の特定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ee35f-40f8-45ae-beaf-fbebcee7cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_position_num = 0\n",
    "for sample in tqdm(dataset_test):\n",
    "    wrong_text = sample['wrong_text']\n",
    "    correct_text = sample['correct_text']\n",
    "    \n",
    "    #符号化\n",
    "    encoding = tokenizer(wrong_text)\n",
    "    wrong_input_ids = encoding['input_ids']\n",
    "    correct_encoding = tokenizer(correct_text)\n",
    "    correct_input_ids = correct_encoding['input_ids']\n",
    "    \n",
    "    #予測\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(**encoding)\n",
    "        scores = output.logits\n",
    "        predict_input_ids = scores[0].argmax(-1).numpy().tolist()\n",
    "        \n",
    "    #特殊トークン除去\n",
    "    wrong_input_ids = wrong_input_ids[1:-1]\n",
    "    correct_input_ids =  correct_input_ids[1:-1]\n",
    "    predict_input_ids =  predict_input_ids[1:-1]\n",
    "    \n",
    "    #特定\n",
    "    detect_flag = True\n",
    "    for wrong_token, correct_token, predict_token \\\n",
    "        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n",
    "        \n",
    "        if wrong_token == correct_token: #正しいトークン\n",
    "            if wrong_token != predict_token:  #変換する必要ないのに変換した\n",
    "                detect_flag = False\n",
    "                break\n",
    "        else:\n",
    "            if wrong_token == predict_token: #誤変換トークン\n",
    "                detect_flag = False　#放置\n",
    "                break\n",
    "                \n",
    "    if detect_flag:\n",
    "        correct_position_num += 1\n",
    "        \n",
    "print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22b972-0334-4c19-9887-dedae06398b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bd779-dc02-4fee-a230-2daf53a2309d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8296811-3022-4fa6-a872-0790003fe523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
